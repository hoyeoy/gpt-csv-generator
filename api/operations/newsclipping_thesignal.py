import requests
from bs4 import BeautifulSoup
from datetime import datetime
import time
from urllib.parse import urljoin
from urllib.parse import quote
import csv

def save_to_csv_with_hyperlink(titles, bodies, urls, filename=None):
    """
    titles: list[str]
    bodies: list[str]
    urls:   list[str]
    â†’ CSV ì €ì¥ + Hyperlink ì»¬ëŸ¼ ì¶”ê°€ (ì—‘ì…€ì—ì„œ í´ë¦­ ê°€ëŠ¥)
    """
    if len(titles) != len(bodies) or len(titles) != len(urls):
        raise ValueError("ëª¨ë“  ë¦¬ìŠ¤íŠ¸ ê¸¸ì´ê°€ ë™ì¼í•´ì•¼ í•©ë‹ˆë‹¤.")

    if not filename:
        today = datetime.now().strftime('%Y%m%d')
        filename = f"sedaily_news_{today}.csv"

    # ì—‘ì…€ì—ì„œ ì¸ì‹í•˜ëŠ” HYPERLINK ê³µì‹: =HYPERLINK("URL", "í…ìŠ¤íŠ¸")
    with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.writer(f)
        
        # í—¤ë”
        writer.writerow(['URL', 'Title', 'Body', 'Hyperlink'])
        
        # ë°ì´í„°
        for url, title, body in zip(urls, titles, bodies):
            # URL ì¸ì½”ë”© (íŠ¹ìˆ˜ë¬¸ì ë°©ì§€)
            safe_url = quote(url, safe=':/?=&%#')
            # í•˜ì´í¼ë§í¬ ê³µì‹ (ì—‘ì…€ì—ì„œ ë°”ë¡œ í´ë¦­ ê°€ëŠ¥)
            hyperlink = f'=HYPERLINK("{safe_url}", "{title.replace('"', '""')}")'
            
            writer.writerow([url, title, body, hyperlink])
    
    print(f"CSV ì €ì¥ ì™„ë£Œ: {filename}")
    print(f"   â†’ ì´ {len(titles)}ê±´")
    print(f"   â†’ ì—‘ì…€ì—ì„œ 'Hyperlink' ì»¬ëŸ¼ í´ë¦­í•˜ë©´ ë°”ë¡œ ì´ë™!")

def get_todays_news_titles():
    """
    signal.sedaily.com ì‹¤ì‹œê°„ ë‰´ìŠ¤ì—ì„œ ì˜¤ëŠ˜ ê²Œì‹œëœ ê¸°ì‚¬ ì œëª©ë§Œ ì¶”ì¶œ
    HTML êµ¬ì¡°ë¥¼ ì‚¬ì´íŠ¸ì— ë§ê²Œ ì¡°ì •: ê°€ì •ëœ êµ¬ì¡° - <li class="article_list_item"><a href="..."><h3 class="title">ì œëª©</h3></a><p class="summary">ìš”ì•½</p><span class="date">ë‚ ì§œ</span></li>
    ì‹¤ì œ êµ¬ì¡°ì— ë§ê²Œ ì„ íƒì ì¡°ì • í•„ìš” (ë¸Œë¼ìš°ì € ê°œë°œì ë„êµ¬ë¡œ í™•ì¸)
    """
    today_str = datetime.now().strftime('%Y-%m-%d')  # "2025-10-27"
    titles = []
    bodies = []
    urls = []
    page = 1
    max_pages = 50
    
    headers = {
        'User-Agent': (
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
            '(KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36'
        )
    }
    
    print(f"ğŸ” {today_str} ê²Œì‹œ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹œì‘...")
    
    base_url = "https://signal.sedaily.com/NewsList/GX11"
    
    while page <= max_pages:
        url = f"{base_url}/page/{page}"  # í˜ì´ì§€ë„¤ì´ì…˜ í˜•ì‹ ê°€ì • (ì‹¤ì œ í™•ì¸ í•„ìš”, ?page={page} ì¼ ìˆ˜ ìˆìŒ)
        try:
            resp = requests.get(url, headers=headers, timeout=10)
            resp.raise_for_status()
            soup = BeautifulSoup(resp.text, 'html.parser')
            
            # âœ… ê¸°ì‚¬ ì•„ì´í…œ ì„ íƒì: ì‚¬ì´íŠ¸ êµ¬ì¡°ì— ë§ê²Œ ì¡°ì • (e.g., 'li' or 'div', class='article_list_item' ë“±)
            article_items = soup.find_all('li', class_='article_list_item')  # í´ë˜ìŠ¤ ì´ë¦„ ì‹¤ì œë¡œ í™•ì¸ í•„ìš”
            
            if not article_items:
                # ëŒ€ì•ˆ ì„ íƒì ì‹œë„
                article_items = soup.find_all('div', class_='news_item')  # ë‹¤ë¥¸ í´ë˜ìŠ¤ ê°€ì •
            if not article_items:
                article_items = soup.find_all('li')  # ëª¨ë“  li fallback
            
            page_has_today = False
            for item in article_items:
                # 1. ë‚ ì§œ ì¶”ì¶œ
                date_span = item.find('span', class_='date')  # í´ë˜ìŠ¤ ì´ë¦„ ì‹¤ì œ í™•ì¸
                if not date_span:
                    continue
                
                date_text = date_span.get_text(strip=True)  # e.g., "2025-10-27 08:14"
                
                # ì˜¤ëŠ˜ ë‚ ì§œì™€ ë§¤ì¹­ (ì‹œì‘ ë¶€ë¶„ ë§¤ì¹­)
                if not date_text.startswith(today_str):
                    continue
                
                # 2. ì œëª© ì¶”ì¶œ: <h3> or <a> íƒœê·¸ ë“±
                title_tag = item.find('h3', class_='title') or item.find('a', class_='title') or item.find('h3') or item.find('a')
                if not title_tag:
                    continue
                
                title = title_tag.get_text(strip=True)
                if title:
                    titles.append(title)
                    page_has_today = True
                    # print(f"âœ… [{page}í˜ì´ì§€] {title}")

                # 3. ìš”ì•½ ì¶”ì¶œ: <p class="summary"> or <div class="excerpt"> ë“±
                body_tag = item.find('p', class_='summary') or item.find('div', class_='body') or item.find('p')
                if not body_tag:
                    body = ''  # ìš”ì•½ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´
                else:
                    body = body_tag.get_text(strip=True)
                    body = body.replace('\n', ' ').replace('\r', ' ').replace('\t', ' ')
                
                bodies.append(body)
                
                # 4. ë§í¬ ì¶”ì¶œ: <a> íƒœê·¸ href
                a_tag = item.find('a', href=True)
                if not a_tag:
                    continue
                
                href = a_tag.get('href')
                if not href:
                    continue
                
                full_url = urljoin(base_url, href)
                urls.append(full_url)
                print(f"âœ… [{page}í˜ì´ì§€] {title} {full_url}")
            
            # í˜„ì¬ í˜ì´ì§€ì— ì˜¤ëŠ˜ ê¸°ì‚¬ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ (ìµœì í™”)
            if not page_has_today and page > 1:
                print(f"â¹ï¸  {page}í˜ì´ì§€ ì´í›„ ì˜¤ëŠ˜ ê¸°ì‚¬ ì—†ìŒ â†’ ì¢…ë£Œ")
                break
            
            if not article_items:
                print(f"â¹ï¸  {page}í˜ì´ì§€: ê¸°ì‚¬ ì—†ìŒ â†’ ì¢…ë£Œ")
                break
                
            print(f"ğŸ“„ {page}í˜ì´ì§€ ì™„ë£Œ (ì´ {len(titles)}ê±´)")
            page += 1
            time.sleep(0.7)  # ì„œë²„ ë¶€í•˜ ë°©ì§€
            
        except requests.RequestException as e:
            print(f"âŒ {page}í˜ì´ì§€ ì˜¤ë¥˜: {e}")
            break
    
    return titles

# ì‹¤í–‰
if __name__ == "__main__":
    titles = []
    bodies = []
    urls = []

    today_titles = get_todays_news_titles()
    
    print("\n" + "="*60)
    print(f"ğŸ‰ {datetime.now().strftime('%Y-%m-%d')} ì´ {len(today_titles)}ê±´ ë‰´ìŠ¤")
    print("="*60)
    
    for i, title in enumerate(today_titles, 1):
        print(f"{i:2d}. {title}")
    
    print("\nâœ… ì™„ë£Œ!")

    # CSV ì €ì¥ + í•˜ì´í¼ë§í¬ ì¶”ê°€
    save_to_csv_with_hyperlink(titles, bodies, urls)

    print("\nâœ… csv íŒŒì¼ ìƒì„± ì™„ë£Œ!")